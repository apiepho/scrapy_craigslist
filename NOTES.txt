
Since this is really a small tool, I am adding this shorter Notes file, rather than a
full blown README.txt and SETUP.txt.

Background
----------

This simple tool uses the Python based scrapy framework to crawl pages from craigslist and
parse all the entries for a given category.

In general, I found working with Python and scrapy more difficult that other web crawl/scrapping
systems, say in Ruby.  Since scrapy is so much faster that Ruby/Watir/Selenium, it cannot
be overlooked.


Installation
------------

Below are the simple command line steps for installing python and scrapy.

brew install python
pip install scrapy
pip install scrapy-splash
mkdir scrapy_craigslist
cd scrapy_craigslist
scrapy startproject scrapy_craigslist scrapy_craigslist


Full documentation on scrapy and installation can be found at:
  https://doc.scrapy.org/en/latest/intro/overview.html

And the details on writing spiders is at:
https://doc.scrapy.org/en/latest/topics/spiders.html


Running
-------

I typically use the following to list the categories:
  scrapy crawl craigslist -a url="https://fortcollins.craigslist.org" -a category=list --nolog

And this to get all the items (form category "Free") into a file "items.csv":
  rm items.csv
  scrapy crawl craigslist -o items.csv -a url="https://fortcollins.craigslist.org" -a category=zip --nolog
  tail items.csv

Alternatively you can give the whole url:
  rm items.csv
  scrapy crawl craigslist -o items.csv -a url="https://fortcollins.craigslist.org/serach/zip" --nolog
  tail items.csv

scrapy generates a lot of log messages by default, so I use --nologs to suppress.  scrapy also
appends to an output file, rather than overwrites it, so I remove it before each run.ÃŸ

Details
-------

Most of the files here are auto generated by the startproject step.  All of the logic is in
...spiders/craigslist.py.